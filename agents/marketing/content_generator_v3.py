"""
ðŸš€ SUPER-AGENT: CONTENT GENERATION ENTERPRISE v5.0 FUSION
Architecture: Event-Driven + CQRS + Self-Healing + Blockchain Audit + Dynamic ML Pipeline
Author: Senior Super-Agent Architect (50 years experience)
"""

import asyncio
import logging
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum
import aiohttp
import redis.asyncio as redis
from contextlib import asynccontextmanager
import hashlib
import pickle
import uuid
import re

# ML Imports - Dynamic Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from xgboost import XGBRegressor, XGBClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
import joblib

# FastAPI/Async
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Query
from pydantic import BaseModel, Field, validator
import warnings
warnings.filterwarnings('ignore')

# ==================== ENUMS & DATA MODELS ====================

class MLStrategy(str, Enum):
    RANDOM_FOREST = "random_forest"
    XGBOOST = "xgboost" 
    GRADIENT_BOOSTING = "gradient_boosting"
    HYBRID = "hybrid"

class ContentType(str, Enum):
    AD_COPY = "ad_copy"
    EMAIL_SUBJECT = "email_subject"
    EMAIL_BODY = "email_body"
    LANDING_HEADLINE = "landing_headline"
    SOCIAL_POST = "social_post"
    SMS = "sms"
    PUSH_NOTIFICATION = "push_notification"

class AudienceSegment(str, Enum):
    HIGH_VALUE = "high_value"
    MID_VALUE = "mid_value"
    LOW_VALUE = "low_value"
    PROSPECT = "prospect"
    DORMANT = "dormant"
    CHURN_RISK = "churn_risk"

class BrandTone(str, Enum):
    PROFESSIONAL = "professional"
    FRIENDLY = "friendly"
    PREMIUM = "premium"
    URGENT = "urgent"
    EDUCATIONAL = "educational"
    CONVERSATIONAL = "conversational"
    AUTHORITATIVE = "authoritative"

class PerformanceTier(str, Enum):
    S = "S"  # Top 5%
    A = "A"  # Top 20%
    B = "B"  # Top 50%
    C = "C"  # Bottom 50%

class HealthStatus(str, Enum):
    OPTIMAL = "optimal"
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    CRITICAL = "critical"

@dataclass
class ContentVariant:
    variant_id: str
    content: str
    length: int
    performance_tier: PerformanceTier
    predicted_ctr: float
    engagement_score: float
    compliance_score: float
    personalization_level: float
    risk_flags: List[str] = field(default_factory=list)
    ml_confidence: float = 1.0
    blockchain_hash: Optional[str] = None
    audit_trail: List[str] = field(default_factory=list)

@dataclass
class BlockchainAuditRecord:
    record_id: str
    content_hash: str
    previous_hash: Optional[str]
    timestamp: datetime
    event_type: str
    metadata: Dict[str, Any]
    signature: str

@dataclass
class AgentHealthMetrics:
    status: HealthStatus
    uptime_seconds: float
    request_count: int
    error_rate: float
    avg_response_time: float
    cache_hit_rate: float
    model_accuracy: float
    circuit_breaker_state: str
    last_self_healing: Optional[datetime]

# ==================== CONFIGURATION MANAGEMENT ====================

class SuperAgentConfig(BaseModel):
    """Enterprise-grade configuration with dynamic ML strategy"""
    
    tenant_id: str = Field(..., min_length=1, max_length=50)
    ml_strategy: MLStrategy = Field(default=MLStrategy.HYBRID)
    enable_self_healing: bool = True
    enable_blockchain_audit: bool = True
    cache_ttl: int = Field(default=3600, ge=300, le=86400)
    rate_limit_per_minute: int = Field(default=1000, ge=100, le=10000)
    
    # ML Configuration
    model_retention_days: int = Field(default=90, ge=7, le=365)
    prediction_confidence_threshold: float = Field(default=0.7, ge=0.5, le=0.95)
    
    # Self-healing thresholds
    max_consecutive_failures: int = Field(default=3, ge=1, le=10)
    health_check_interval: int = Field(default=60, ge=10, le=300)
    
    # Performance tuning
    max_variant_count: int = Field(default=10, ge=1, le=20)
    enable_advanced_personalization: bool = True
    
    @validator('tenant_id')
    def validate_tenant_id(cls, v):
        if not re.match(r"^[a-zA-Z0-9_-]{3,50}$", v):
            raise ValueError("Invalid tenant_id format")
        return v

# ==================== HYBRID CACHE MANAGEMENT ====================

class HybridCacheManager:
    """Redis + In-Memory LRU cache with circuit breaker and self-healing"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379", max_memory_size: int = 1000):
        self.redis_url = redis_url
        self.redis_pool = None
        self.memory_cache: OrderedDict = OrderedDict()
        self.max_memory_size = max_memory_size
        self.circuit_state = "CLOSED"
        self.failure_count = 0
        self.health_metrics = {
            "redis_available": True,
            "memory_cache_hits": 0,
            "redis_cache_hits": 0,
            "total_requests": 0
        }
        
    async def initialize(self):
        """Initialize both Redis and memory cache"""
        try:
            self.redis_pool = redis.ConnectionPool.from_url(
                self.redis_url, max_connections=20
            )
            async with redis.Redis(connection_pool=self.redis_pool) as r:
                await r.ping()
            self.circuit_state = "CLOSED"
            self.failure_count = 0
            self.health_metrics["redis_available"] = True
        except Exception as e:
            logging.warning(f"Redis initialization failed, using memory cache only: {e}")
            self.circuit_state = "OPEN"
            self.health_metrics["redis_available"] = False
            
    async def get(self, key: str) -> Optional[Any]:
        """Get from cache with hybrid strategy"""
        self.health_metrics["total_requests"] += 1
        
        # Try memory cache first
        if key in self.memory_cache:
            self.health_metrics["memory_cache_hits"] += 1
            value, timestamp = self.memory_cache[key]
            # Check TTL
            if (datetime.now().timestamp() - timestamp) < 3600:  # 1 hour TTL for memory
                self.memory_cache.move_to_end(key)
                return value
            else:
                del self.memory_cache[key]
        
        # Try Redis if available
        if self.health_metrics["redis_available"] and self.circuit_state == "CLOSED":
            try:
                async with redis.Redis(connection_pool=self.redis_pool) as conn:
                    cached = await conn.get(f"content:{key}")
                    if cached:
                        value = pickle.loads(cached)
                        # Also store in memory cache for faster access
                        self._set_memory(key, value)
                        self.health_metrics["redis_cache_hits"] += 1
                        return value
            except Exception as e:
                self.failure_count += 1
                if self.failure_count >= 3:
                    self.circuit_state = "OPEN"
                    asyncio.create_task(self._reset_circuit_breaker())
        
        return None
        
    async def set(self, key: str, value: Any, ttl: int = 3600):
        """Set in both caches with TTL"""
        # Always set in memory cache
        self._set_memory(key, value)
        
        # Set in Redis if available
        if self.health_metrics["redis_available"] and self.circuit_state == "CLOSED":
            try:
                async with redis.Redis(connection_pool=self.redis_pool) as conn:
                    await conn.setex(
                        f"content:{key}", 
                        ttl, 
                        pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)
                    )
            except Exception as e:
                self.failure_count += 1
                if self.failure_count >= 3:
                    self.circuit_state = "OPEN"
                    asyncio.create_task(self._reset_circuit_breaker())
                    
    def _set_memory(self, key: str, value: Any):
        """Set in memory cache with LRU eviction"""
        self.memory_cache[key] = (value, datetime.now().timestamp())
        if len(self.memory_cache) > self.max_memory_size:
            self.memory_cache.popitem(last=False)
            
    async def _reset_circuit_breaker(self):
        """Reset circuit breaker after timeout"""
        await asyncio.sleep(60)
        self.circuit_state = "HALF_OPEN"
        self.failure_count = 0

# ==================== DYNAMIC ML PIPELINE ====================

class DynamicMLPipeline:
    """ML pipeline with strategy pattern and self-optimization"""
    
    def __init__(self, strategy: MLStrategy = MLStrategy.HYBRID):
        self.strategy = strategy
        self.active_models: Dict[str, Any] = {}
        self.vectorizers: Dict[str, Any] = {}
        self.scalers: Dict[str, Any] = {}
        self.performance_history: Dict[str, List[float]] = {}
        self.model_metrics: Dict[str, Dict] = {}
        
    async def initialize(self):
        """Initialize models based on strategy"""
        if self.strategy == MLStrategy.RANDOM_FOREST:
            await self._initialize_random_forest()
        elif self.strategy == MLStrategy.XGBOOST:
            await self._initialize_xgboost()
        elif self.strategy == MLStrategy.GRADIENT_BOOSTING:
            await self._initialize_gradient_boosting()
        else:  # HYBRID
            await self._initialize_hybrid()
            
        # Initialize common components
        self.vectorizers["content_embedding"] = TfidfVectorizer(
            max_features=1000,
            stop_words=['english', 'spanish', 'portuguese'],
            ngram_range=(1, 3)
        )
        self.scalers = {
            "engagement_features": StandardScaler(),
            "ctr_features": RobustScaler()
        }
        
    async def _initialize_random_forest(self):
        """Initialize Random Forest models"""
        self.active_models = {
            "ctr_prediction": RandomForestRegressor(
                n_estimators=200, max_depth=15, random_state=42, n_jobs=-1
            ),
            "engagement_predictor": RandomForestRegressor(
                n_estimators=150, max_depth=12, random_state=42, n_jobs=-1
            ),
            "compliance_scorer": RandomForestRegressor(
                n_estimators=100, max_depth=10, random_state=42
            )
        }
        
    async def _initialize_xgboost(self):
        """Initialize XGBoost models"""
        self.active_models = {
            "ctr_prediction": XGBRegressor(
                n_estimators=200, max_depth=8, learning_rate=0.1, random_state=42
            ),
            "engagement_predictor": XGBClassifier(
                n_estimators=150, max_depth=6, learning_rate=0.1, random_state=42
            ),
            "compliance_scorer": XGBRegressor(
                n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42
            )
        }
        
    async def _initialize_gradient_boosting(self):
        """Initialize Gradient Boosting models"""
        self.active_models = {
            "ctr_prediction": GradientBoostingClassifier(
                n_estimators=200, max_depth=8, random_state=42
            ),
            "engagement_predictor": GradientBoostingClassifier(
                n_estimators=150, max_depth=6, random_state=42
            ),
            "compliance_scorer": GradientBoostingClassifier(
                n_estimators=100, max_depth=6, random_state=42
            )
        }
        
    async def _initialize_hybrid(self):
        """Initialize hybrid model strategy"""
        self.active_models = {
            "ctr_prediction": XGBRegressor(
                n_estimators=200, max_depth=8, learning_rate=0.1, random_state=42
            ),
            "engagement_predictor": RandomForestRegressor(
                n_estimators=150, max_depth=12, random_state=42, n_jobs=-1
            ),
            "compliance_scorer": GradientBoostingClassifier(
                n_estimators=100, max_depth=6, random_state=42
            )
        }
        
    async def predict_content_performance(self, 
                                        content_features: np.ndarray,
                                        content_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Advanced performance prediction with confidence scoring"""
        try:
            # Feature preprocessing
            features_scaled = self.scalers["ctr_features"].transform(
                content_features.reshape(1, -1)
            )
            
            # Ensemble predictions
            ctr_prediction = self.active_models["ctr_prediction"].predict(features_scaled)[0]
            engagement_proba = self.active_models["engagement_predictor"].predict_proba(features_scaled)[0]
            compliance_score = self.active_models["compliance_scorer"].predict(features_scaled)[0]
            
            # Calculate confidence intervals
            confidence = self._calculate_prediction_confidence(
                ctr_prediction, np.max(engagement_proba), compliance_score
            )
            
            # Determine performance tier
            performance_tier = self._calculate_performance_tier(
                ctr_prediction, np.max(engagement_proba), compliance_score
            )
            
            return {
                "predicted_ctr": float(ctr_prediction),
                "engagement_score": float(np.max(engagement_proba)),
                "compliance_score": float(compliance_score),
                "performance_tier": performance_tier,
                "confidence": confidence,
                "model_strategy": self.strategy.value
            }
            
        except Exception as e:
            logging.error(f"ML prediction failed: {e}")
            return self._get_fallback_predictions()
            
    def _calculate_prediction_confidence(self, ctr: float, engagement: float, compliance: float) -> float:
        """Calculate prediction confidence based on variance and model performance"""
        # Simple confidence calculation - can be enhanced with proper uncertainty quantification
        scores = [ctr, engagement, compliance]
        variance = np.var(scores)
        confidence = 1.0 - min(variance * 2, 0.5)  # Reduce confidence with high variance
        return max(0.5, min(1.0, confidence))
        
    def _calculate_performance_tier(self, ctr: float, engagement: float, compliance: float) -> PerformanceTier:
        """Calculate performance tier with dynamic thresholds"""
        overall_score = (ctr * 0.4 + engagement * 0.4 + compliance * 0.2)
        
        if overall_score >= 0.85: return PerformanceTier.S
        elif overall_score >= 0.70: return PerformanceTier.A
        elif overall_score >= 0.50: return PerformanceTier.B
        else: return PerformanceTier.C
        
    def _get_fallback_predictions(self) -> Dict[str, Any]:
        """Fallback predictions when ML models fail"""
        return {
            "predicted_ctr": 0.15,
            "engagement_score": 0.5,
            "compliance_score": 0.9,
            "performance_tier": PerformanceTier.C,
            "confidence": 0.5,
            "model_strategy": "fallback"
        }
        
    async def retrain_models(self, training_data: pd.DataFrame, target_metrics: Dict[str, Any]):
        """Retrain models with new data and update performance metrics"""
        # Implementation for online learning would go here
        pass

# ==================== BLOCKCHAIN AUDIT SYSTEM ====================

class BlockchainAuditSystem:
    """Immutable audit trail with blockchain-like structure"""
    
    def __init__(self):
        self.chain: List[BlockchainAuditRecord] = []
        self.last_hash: Optional[str] = None
        
    async def add_record(self, event_type: str, content: str, metadata: Dict[str, Any]) -> str:
        """Add a new record to the blockchain audit trail"""
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        record_id = str(uuid.uuid4())
        
        # Create digital signature
        signature = self._create_signature(content_hash, metadata)
        
        record = BlockchainAuditRecord(
            record_id=record_id,
            content_hash=content_hash,
            previous_hash=self.last_hash,
            timestamp=datetime.now(),
            event_type=event_type,
            metadata=metadata,
            signature=signature
        )
        
        self.chain.append(record)
        self.last_hash = self._calculate_block_hash(record)
        
        logging.info(f"Blockchain audit record added: {event_type} - {record_id}")
        return record_id
        
    def _create_signature(self, content_hash: str, metadata: Dict[str, Any]) -> str:
        """Create digital signature for audit record"""
        signature_data = f"{content_hash}{json.dumps(metadata, sort_keys=True)}{datetime.now().timestamp()}"
        return hashlib.sha256(signature_data.encode()).hexdigest()
        
    def _calculate_block_hash(self, record: BlockchainAuditRecord) -> str:
        """Calculate hash for a block"""
        block_data = f"{record.record_id}{record.content_hash}{record.previous_hash}{record.timestamp.isoformat()}{record.signature}"
        return hashlib.sha256(block_data.encode()).hexdigest()
        
    def verify_chain_integrity(self) -> bool:
        """Verify the integrity of the entire blockchain"""
        for i in range(1, len(self.chain)):
            current_block = self.chain[i]
            previous_block = self.chain[i-1]
            
            if current_block.previous_hash != self._calculate_block_hash(previous_block):
                return False
                
            # Verify signature
            expected_signature = self._create_signature(
                current_block.content_hash, current_block.metadata
            )
            if current_block.signature != expected_signature:
                return False
                
        return True
        
    def get_audit_trail(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get recent audit trail entries"""
        recent_records = self.chain[-limit:] if self.chain else []
        return [
            {
                "record_id": r.record_id,
                "event_type": r.event_type,
                "timestamp": r.timestamp.isoformat(),
                "content_hash": r.content_hash,
                "metadata": r.metadata
            }
            for r in recent_records
        ]

# ==================== SELF-HEALING ORCHESTRATOR ====================

class SelfHealingOrchestrator:
    """Advanced self-healing system with health monitoring"""
    
    def __init__(self, max_consecutive_failures: int = 3, check_interval: int = 60):
        self.max_consecutive_failures = max_consecutive_failures
        self.check_interval = check_interval
        self.consecutive_failures = 0
        self.health_metrics: Dict[str, Any] = {}
        self.last_health_check = datetime.now()
        self.self_healing_history: List[Dict[str, Any]] = []
        
    async def monitor_health(self, agent) -> HealthStatus:
        """Monitor agent health and trigger healing if needed"""
        current_time = datetime.now()
        
        # Check if it's time for health check
        if (current_time - self.last_health_check).seconds < self.check_interval:
            return HealthStatus.HEALTHY
            
        self.last_health_check = current_time
        
        # Perform health checks
        health_status = await self._perform_health_checks(agent)
        
        # Trigger self-healing if status is degraded or critical
        if health_status in [HealthStatus.DEGRADED, HealthStatus.CRITICAL]:
            await self._trigger_self_healing(agent, health_status)
            
        return health_status
        
    async def _perform_health_checks(self, agent) -> HealthStatus:
        """Perform comprehensive health checks"""
        checks = {
            "cache_health": await self._check_cache_health(agent.cache_manager),
            "ml_health": await self._check_ml_health(agent.ml_pipeline),
            "api_health": await self._check_api_health(agent),
            "memory_health": await self._check_memory_health()
        }
        
        # Calculate overall health score
        successful_checks = sum(1 for check in checks.values() if check)
        health_ratio = successful_checks / len(checks)
        
        if health_ratio >= 0.9: return HealthStatus.OPTIMAL
        elif health_ratio >= 0.7: return HealthStatus.HEALTHY
        elif health_ratio >= 0.5: return HealthStatus.DEGRADED
        else: return HealthStatus.CRITICAL
        
    async def _check_cache_health(self, cache_manager) -> bool:
        """Check cache system health"""
        try:
            # Test cache operations
            test_key = "health_check"
            test_value = {"test": "data", "timestamp": datetime.now().isoformat()}
            await cache_manager.set(test_key, test_value, 60)
            retrieved = await cache_manager.get(test_key)
            return retrieved is not None
        except:
            return False
            
    async def _check_ml_health(self, ml_pipeline) -> bool:
        """Check ML pipeline health"""
        try:
            # Test with sample data
            sample_features = np.random.rand(100)
            prediction = await ml_pipeline.predict_content_performance(
                sample_features, {}
            )
            return prediction["confidence"] > 0
        except:
            return False
            
    async def _check_api_health(self, agent) -> bool:
        """Check API health"""
        try:
            status = await agent.get_agent_status()
            return status["status"] == "OPERATIONAL"
        except:
            return False
            
    async def _check_memory_health(self) -> bool:
        """Check memory usage health"""
        try:
            import psutil
            memory_percent = psutil.virtual_memory().percent
            return memory_percent < 90  # Healthy if memory usage < 90%
        except:
            return True  # If we can't check, assume healthy
            
    async def _trigger_self_healing(self, agent, health_status: HealthStatus):
        """Trigger self-healing procedures"""
        healing_actions = []
        
        # Reset cache if problematic
        if not await self._check_cache_health(agent.cache_manager):
            await agent.cache_manager.initialize()
            healing_actions.append("cache_reset")
            
        # Reinitialize ML models if problematic
        if not await self._check_ml_health(agent.ml_pipeline):
            await agent.ml_pipeline.initialize()
            healing_actions.append("ml_pipeline_reset")
            
        # Log healing event
        healing_event = {
            "timestamp": datetime.now().isoformat(),
            "health_status": health_status.value,
            "actions_taken": healing_actions,
            "consecutive_failures": self.consecutive_failures
        }
        self.self_healing_history.append(healing_event)
        
        logging.warning(f"Self-healing triggered: {healing_actions}")
        
        # Reset failure count after healing
        self.consecutive_failures = 0

# ==================== CONTENT GENERATION SUPER-AGENT v5.0 ====================

class ContentGenerationSuperAgent:
    """
    ðŸš€ ENTERPRISE SUPER-AGENT v5.0 FUSION
    Advanced AI-Powered Content Generation with Self-Healing & Blockchain Audit
    """
    
    def __init__(self, tenant_id: str, config: SuperAgentConfig):
        self.tenant_id = tenant_id
        self.config = config
        self.agent_id = f"content_super_{tenant_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = datetime.now()
        
        # Initialize advanced components
        self.cache_manager = HybridCacheManager()
        self.ml_pipeline = DynamicMLPipeline(config.ml_strategy)
        self.audit_system = BlockchainAuditSystem()
        self.self_healing = SelfHealingOrchestrator(
            max_consecutive_failures=config.max_consecutive_failures,
            check_interval=config.health_check_interval
        )
        
        # Enhanced template engine
        self.template_engine = DynamicTemplateEngine()
        self.compliance_engine = ComplianceEngineEnterprise()
        
        # Event system
        self.event_queue = asyncio.Queue(maxsize=2000)
        
        # Advanced metrics
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "content_variants_generated": 0,
            "cache_hit_rate": 0.0,
            "avg_processing_time_ms": 0.0,
            "ml_predictions_made": 0,
            "compliance_checks": 0,
            "self_healing_events": 0,
            "blockchain_audit_records": 0
        }
        
        # Performance tracking
        self.performance_history = {
            "response_times": [],
            "error_rates": [],
            "cache_performance": [],
            "model_accuracy": []
        }
        
        self.background_tasks = set()
        self.is_running = True
        
        logging.info(f"ContentGenerationSuperAgent v5.0 initialized for tenant {tenant_id}")
        
    async def initialize(self):
        """Initialize all components with error handling"""
        try:
            await self.cache_manager.initialize()
            await self.ml_pipeline.initialize()
            await self.compliance_engine.initialize()
            
            # Start background tasks
            tasks = [
                self._process_events(),
                self._health_monitoring(),
                self._performance_optimization()
            ]
            
            for task in tasks:
                bg_task = asyncio.create_task(task)
                self.background_tasks.add(bg_task)
                bg_task.add_done_callback(self.background_tasks.discard)
                
            logging.info(f"Super Agent {self.agent_id} fully initialized")
            
        except Exception as e:
            logging.error(f"Agent initialization failed: {e}")
            raise
            
    async def shutdown(self):
        """Graceful shutdown with state preservation"""
        self.is_running = False
        await asyncio.gather(*self.background_tasks, return_exceptions=True)
        
        # Save state if needed
        await self._save_agent_state()
        
    async def generate_content(self,
                            content_type: ContentType,
                            audience_segment: AudienceSegment,
                            brand_tone: BrandTone,
                            key_message: str,
                            personalization_data: Dict[str, Any] = None,
                            variant_count: int = 5,
                            request_id: str = None) -> Dict[str, Any]:
        """
        Generate optimized content with advanced features
        """
        start_time = datetime.now()
        request_id = request_id or self._generate_request_id()
        
        try:
            # Health check
            health_status = await self.self_healing.monitor_health(self)
            if health_status == HealthStatus.CRITICAL:
                raise RuntimeError("Agent health is critical, self-healing in progress")
                
            # Check cache
            cache_key = self._generate_cache_key(
                content_type, audience_segment, brand_tone, key_message, personalization_data
            )
            cached_result = await self.cache_manager.get(cache_key)
            
            if cached_result:
                self.metrics["cache_hit_rate"] = (
                    self.metrics["cache_hit_rate"] * 0.95 + 0.05
                )
                return cached_result
                
            # Generate content variants
            variants_data = await self.template_engine.generate_variants(
                base_template=key_message,
                variant_count=min(variant_count, self.config.max_variant_count),
                personalization_context=personalization_data or {},
                brand_tone=brand_tone,
                content_type=content_type
            )
            
            # Process variants with ML and compliance
            final_variants = []
            for variant in variants_data:
                # ML performance prediction
                ml_prediction = await self.ml_pipeline.predict_content_performance(
                    content_features=variant["embedding"],
                    content_metadata={
                        "content_type": content_type.value,
                        "audience_segment": audience_segment.value,
                        "brand_tone": brand_tone.value
                    }
                )
                
                # Compliance checking
                compliance_score, risk_flags = await self.compliance_engine.check_content_compliance(
                    content=variant["content"],
                    jurisdiction=self.tenant_id,  # Using tenant as jurisdiction
                    content_type=content_type.value
                )
                
                # Blockchain audit
                audit_id = await self.audit_system.add_record(
                    event_type="content_generation",
                    content=variant["content"],
                    metadata={
                        "variant_id": variant["variant_id"],
                        "content_type": content_type.value,
                        "compliance_score": compliance_score,
                        "risk_flags": risk_flags
                    }
                )
                
                # Create final variant
                content_variant = ContentVariant(
                    variant_id=variant["variant_id"],
                    content=variant["content"],
                    length=variant["length"],
                    performance_tier=ml_prediction["performance_tier"],
                    predicted_ctr=ml_prediction["predicted_ctr"],
                    engagement_score=ml_prediction["engagement_score"],
                    compliance_score=compliance_score,
                    personalization_level=variant["personalization_level"],
                    risk_flags=risk_flags,
                    ml_confidence=ml_prediction["confidence"],
                    blockchain_hash=audit_id,
                    audit_trail=[f"audit:{audit_id}", f"ml_confidence:{ml_prediction['confidence']:.2f}"]
                )
                final_variants.append(content_variant)
                
            # Select best variant using advanced scoring
            recommended_variant = self._select_optimal_variant(final_variants, audience_segment)
            
            processing_time_ms = (datetime.now() - start_time).total_seconds() * 1000
            
            # Build result
            result = {
                "request_id": request_id,
                "tenant_id": self.tenant_id,
                "generation_timestamp": datetime.now().isoformat(),
                "processing_time_ms": processing_time_ms,
                "health_status": health_status.value,
                "variants": [
                    {
                        "variant_id": v.variant_id,
                        "content": v.content,
                        "length": v.length,
                        "performance_tier": v.performance_tier.value,
                        "predicted_ctr": v.predicted_ctr,
                        "engagement_score": v.engagement_score,
                        "compliance_score": v.compliance_score,
                        "personalization_level": v.personalization_level,
                        "risk_flags": v.risk_flags,
                        "ml_confidence": v.ml_confidence,
                        "blockchain_hash": v.blockchain_hash
                    }
                    for v in final_variants
                ],
                "recommended_variant": recommended_variant.variant_id,
                "performance_summary": {
                    "best_ctr": recommended_variant.predicted_ctr,
                    "best_engagement": recommended_variant.engagement_score,
                    "overall_risk_level": self._calculate_risk_level(final_variants),
                    "average_confidence": np.mean([v.ml_confidence for v in final_variants])
                },
                "metadata": {
                    "agent_version": "5.0.0-fusion",
                    "ml_strategy": self.config.ml_strategy.value,
                    "cache_strategy": "hybrid_redis_memory",
                    "blockchain_audit_enabled": self.config.enable_blockchain_audit,
                    "self_healing_enabled": self.config.enable_self_healing
                }
            }
            
            # Cache result
            await self.cache_manager.set(cache_key, result, self.config.cache_ttl)
            
            # Update metrics
            self._update_metrics(processing_time_ms, len(final_variants), True)
            
            # Emit success event
            await self._emit_event("generation_success", {
                "request_id": request_id,
                "variant_count": len(final_variants),
                "processing_time_ms": processing_time_ms,
                "health_status": health_status.value
            })
            
            return result
            
        except Exception as e:
            # Update error metrics
            self._update_metrics(0, 0, False)
            
            # Emit error event
            await self._emit_event("generation_failed", {
                "request_id": request_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            
            logging.error(f"Content generation failed for request {request_id}: {e}")
            raise HTTPException(
                status_code=500, 
                detail=f"Content generation failed: {str(e)}"
            )
            
    def _select_optimal_variant(self, variants: List[ContentVariant], audience_segment: AudienceSegment) -> ContentVariant:
        """Select optimal variant with audience-specific weighting"""
        
        def variant_score(variant: ContentVariant) -> float:
            # Base weights
            weights = {
                "ctr": 0.35,
                "engagement": 0.30,
                "compliance": 0.25,
                "personalization": 0.10
            }
            
            # Adjust weights based on audience segment
            if audience_segment == AudienceSegment.HIGH_VALUE:
                weights.update({"engagement": 0.40, "ctr": 0.25})
            elif audience_segment == AudienceSegment.CHURN_RISK:
                weights.update({"personalization": 0.20, "engagement": 0.35})
                
            # Calculate weighted score
            score = (
                variant.predicted_ctr * weights["ctr"] +
                variant.engagement_score * weights["engagement"] +
                variant.compliance_score * weights["compliance"] +
                variant.personalization_level * weights["personalization"]
            )
            
            # Apply confidence multiplier
            score *= variant.ml_confidence
            
            return score
            
        return max(variants, key=variant_score)
        
    def _calculate_risk_level(self, variants: List[ContentVariant]) -> str:
        """Calculate overall risk level"""
        risk_scores = []
        for variant in variants:
            # Base risk from flags
            base_risk = len(variant.risk_flags) * 0.2
            # Compliance risk
            compliance_risk = (1 - variant.compliance_score) * 0.5
            # Confidence risk
            confidence_risk = (1 - variant.ml_confidence) * 0.3
            
            total_risk = base_risk + compliance_risk + confidence_risk
            risk_scores.append(total_risk)
            
        avg_risk = np.mean(risk_scores)
        
        if avg_risk >= 0.7: return "HIGH"
        elif avg_risk >= 0.4: return "MEDIUM"
        else: return "LOW"
        
    def _generate_cache_key(self, 
                          content_type: ContentType,
                          audience_segment: AudienceSegment, 
                          brand_tone: BrandTone,
                          key_message: str,
                          personalization_data: Dict[str, Any]) -> str:
        """Generate cache key with context"""
        key_components = [
            self.tenant_id,
            content_type.value,
            audience_segment.value,
            brand_tone.value,
            key_message,
            json.dumps(personalization_data, sort_keys=True) if personalization_data else "",
            self.config.ml_strategy.value  # Include ML strategy in cache key
        ]
        key_str = "|".join(key_components)
        return hashlib.sha256(key_str.encode()).hexdigest()[:20]
        
    def _generate_request_id(self) -> str:
        """Generate unique request ID"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S%f")
        return f"req_{self.tenant_id}_{timestamp}_{hashlib.sha256(timestamp.encode()).hexdigest()[:8]}"
        
    def _update_metrics(self, processing_time_ms: float, variant_count: int, success: bool):
        """Update comprehensive metrics"""
        self.metrics["total_requests"] += 1
        
        if success:
            self.metrics["successful_requests"] += 1
            self.metrics["content_variants_generated"] += variant_count
            self.metrics["ml_predictions_made"] += variant_count
            self.metrics["compliance_checks"] += variant_count
            
            # Update average processing time (EMA)
            current_avg = self.metrics["avg_processing_time_ms"]
            if current_avg == 0:
                self.metrics["avg_processing_time_ms"] = processing_time_ms
            else:
                self.metrics["avg_processing_time_ms"] = current_avg * 0.95 + processing_time_ms * 0.05
        else:
            self.metrics["failed_requests"] += 1
            
        # Update cache hit rate
        total = self.metrics["total_requests"]
        hits = self.metrics["total_requests"] - self.metrics["failed_requests"]  # Simplified
        self.metrics["cache_hit_rate"] = hits / total if total > 0 else 0.0
        
    async def _emit_event(self, event_type: str, payload: Dict[str, Any]):
        """Emit event to event bus"""
        event = {
            "event_id": self._generate_request_id(),
            "event_type": event_type,
            "timestamp": datetime.now().isoformat(),
            "tenant_id": self.tenant_id,
            "agent_id": self.agent_id,
            "payload": payload
        }
        await self.event_queue.put(event)
        
    async def _process_events(self):
        """Background task to process events"""
        while self.is_running:
            try:
                event = await asyncio.wait_for(self.event_queue.get(), timeout=1.0)
                # Process event (analytics, monitoring, etc.)
                await self._handle_event(event)
                self.event_queue.task_done()
            except asyncio.TimeoutError:
                continue
                
    async def _handle_event(self, event: Dict[str, Any]):
        """Handle different event types"""
        event_type = event["event_type"]
        
        if event_type == "generation_success":
            # Update performance history
            self.performance_history["response_times"].append(
                event["payload"]["processing_time_ms"]
            )
            # Keep only last 1000 records
            if len(self.performance_history["response_times"]) > 1000:
                self.performance_history["response_times"].pop(0)
                
        elif event_type == "generation_failed":
            self.performance_history["error_rates"].append(1)
            if len(self.performance_history["error_rates"]) > 1000:
                self.performance_history["error_rates"].pop(0)
                
    async def _health_monitoring(self):
        """Continuous health monitoring"""
        while self.is_running:
            try:
                health_status = await self.self_healing.monitor_health(self)
                
                # Log health status periodically
                if health_status != HealthStatus.OPTIMAL:
                    logging.info(f"Agent health status: {health_status.value}")
                    
                await asyncio.sleep(self.config.health_check_interval)
                
            except Exception as e:
                logging.error(f"Health monitoring error: {e}")
                await asyncio.sleep(30)  # Wait before retrying
                
    async def _performance_optimization(self):
        """Continuous performance optimization"""
        while self.is_running:
            try:
                # Analyze performance trends and optimize
                await self._analyze_performance_trends()
                await asyncio.sleep(300)  # Every 5 minutes
            except Exception as e:
                logging.error(f"Performance optimization error: {e}")
                await asyncio.sleep(60)
                
    async def _analyze_performance_trends(self):
        """Analyze performance trends and make adjustments"""
        # Implementation for dynamic optimization based on performance data
        pass
        
    async def _save_agent_state(self):
        """Save agent state for recovery"""
        # Implementation for state persistence
        pass
        
    async def get_agent_status(self) -> Dict[str, Any]:
        """Get comprehensive agent status with health metrics"""
        uptime = (datetime.now() - self.start_time).total_seconds()
        
        health_metrics = AgentHealthMetrics(
            status=await self.self_healing.monitor_health(self),
            uptime_seconds=uptime,
            request_count=self.metrics["total_requests"],
            error_rate=self.metrics["failed_requests"] / max(1, self.metrics["total_requests"]),
            avg_response_time=self.metrics["avg_processing_time_ms"],
            cache_hit_rate=self.metrics["cache_hit_rate"],
            model_accuracy=0.85,  # Would be calculated from validation data
            circuit_breaker_state=self.cache_manager.circuit_state,
            last_self_healing=(
                self.self_healing.self_healing_history[-1]["timestamp"] 
                if self.self_healing.self_healing_history else None
            )
        )
        
        return {
            "agent_id": self.agent_id,
            "tenant_id": self.tenant_id,
            "status": "OPERATIONAL",
            "health_metrics": asdict(health_metrics),
            "performance_metrics": self.metrics,
            "cache_status": {
                "circuit_state": self.cache_manager.circuit_state,
                "redis_available": self.cache_manager.health_metrics["redis_available"],
                "memory_cache_size": len(self.cache_manager.memory_cache)
            },
            "ml_pipeline_status": {
                "strategy": self.config.ml_strategy.value,
                "models_loaded": len(self.ml_pipeline.active_models),
                "last_training": "N/A"  # Would track training timestamps
            },
            "audit_system_status": {
                "blockchain_records": len(self.audit_system.chain),
                "chain_integrity": self.audit_system.verify_chain_integrity(),
                "last_audit_record": self.audit_system.chain[-1].timestamp.isoformat() if self.audit_system.chain else None
            },
            "self_healing_status": {
                "enabled": self.config.enable_self_healing,
                "consecutive_failures": self.self_healing.consecutive_failures,
                "healing_events": len(self.self_healing.self_healing_history),
                "last_healing": self.self_healing.self_healing_history[-1]["timestamp"] if self.self_healing.self_healing_history else None
            },
            "capabilities": [
                "AI-powered content generation",
                "Dynamic ML pipeline with multiple strategies",
                "Hybrid caching (Redis + Memory)",
                "Blockchain audit trail",
                "Self-healing orchestration",
                "Real-time health monitoring",
                "Advanced personalization",
                "Multi-tenant enterprise readiness"
            ],
            "version": "5.0.0-fusion-enterprise"
        }

# ==================== FASTAPI INTEGRATION ====================

app = FastAPI(
    title="Content Generation Super-Agent v5.0 Fusion",
    version="5.0.0",
    description="Enterprise AI-Powered Content Generation with Self-Healing & Blockchain Audit"
)

# Global agent registry with tenant isolation
agent_registry: Dict[str, ContentGenerationSuperAgent] = {}

@app.on_event("startup")
async def startup_event():
    """Initialize default agent on startup"""
    config = SuperAgentConfig(tenant_id="default")
    agent = ContentGenerationSuperAgent("default", config)
    await agent.initialize()
    agent_registry["default"] = agent
    logging.info("Content Generation Super-Agent v5.0 Fusion started successfully")

@app.on_event("shutdown")
async def shutdown_event():
    """Shutdown all agents gracefully"""
    for agent in agent_registry.values():
        await agent.shutdown()

@app.post("/api/v1/generate")
async def generate_content(
    content_type: ContentType,
    audience_segment: AudienceSegment,
    brand_tone: BrandTone,
    key_message: str = Query(..., min_length=10, max_length=500),
    personalization_data: Dict[str, Any] = None,
    variant_count: int = Query(default=5, ge=1, le=10),
    tenant_id: str = "default"
):
    """Enterprise content generation endpoint"""
    agent = agent_registry.get(tenant_id)
    if not agent:
        raise HTTPException(status_code=404, detail=f"Agent not found for tenant: {tenant_id}")
        
    return await agent.generate_content(
        content_type=content_type,
        audience_segment=audience_segment,
        brand_tone=brand_tone,
        key_message=key_message,
        personalization_data=personalization_data,
        variant_count=variant_count
    )

@app.get("/api/v1/status/{tenant_id}")
async def get_agent_status(tenant_id: str):
    """Get comprehensive agent status"""
    agent = agent_registry.get(tenant_id)
    if not agent:
        raise HTTPException(status_code=404, detail=f"Agent not found for tenant: {tenant_id}")
    return await agent.get_agent_status()

@app.get("/api/v1/audit/trail/{tenant_id}")
async def get_audit_trail(tenant_id: str, limit: int = Query(default=50, ge=1, le=1000)):
    """Get blockchain audit trail"""
    agent = agent_registry.get(tenant_id)
    if not agent:
        raise HTTPException(status_code=404, detail=f"Agent not found for tenant: {tenant_id}")
    return agent.audit_system.get_audit_trail(limit)

@app.post("/api/v1/agents/{tenant_id}/heal")
async def trigger_healing(tenant_id: str):
    """Trigger self-healing manually"""
    agent = agent_registry.get(tenant_id)
    if not agent:
        raise HTTPException(status_code=404, detail=f"Agent not found for tenant: {tenant_id}")
    
    # Force health check and healing
    health_status = await agent.self_healing.monitor_health(agent)
    await agent.self_healing._trigger_self_healing(agent, health_status)
    
    return {"status": "healing_triggered", "health_status": health_status.value}

@app.get("/health")
async def health_check():
    """Comprehensive health check"""
    health_status = {}
    for tenant_id, agent in agent_registry.items():
        try:
            status = await agent.get_agent_status()
            health_status[tenant_id] = status["health_metrics"]["status"]
        except:
            health_status[tenant_id] = "UNKNOWN"
            
    overall_health = (
        "healthy" if all(status == "optimal" for status in health_status.values())
        else "degraded" if any(status == "degraded" for status in health_status.values())
        else "critical"
    )
    
    return {
        "status": overall_health,
        "timestamp": datetime.now().isoformat(),
        "tenants": health_status,
        "version": "5.0.0-fusion"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")